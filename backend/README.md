# ATS Resume Analyzer - Backend

Production-grade FastAPI backend for ATS Resume Analyzer with semantic ranking and RAG explainability.

## ğŸš€ Features

- **FastAPI** REST API with automatic OpenAPI docs
- **JWT Authentication** + **Google OAuth 2.0**
- **PDF Processing** with pdfplumber and PyPDF2 fallback
- **OpenAI Embeddings** using text-embedding-3-small
- **ChromaDB** vector database for semantic search
- **Deterministic Ranking Engine** with weighted scoring
- **RAG Explainability** using LangChain + GPT-3.5-turbo
- **PostgreSQL** database with SQLAlchemy ORM

## ğŸ“‹ Prerequisites

- Python 3.9+
- PostgreSQL database
- OpenAI API key
- Google OAuth credentials (optional)

## ğŸ› ï¸ Setup

### 1. Create Virtual Environment

```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Environment

Copy `.env.example` to `.env` and fill in your credentials:

```bash
cp .env.example .env
```

Required environment variables:
- `DATABASE_URL`: PostgreSQL connection string
- `OPENAI_API_KEY`: Your OpenAI API key
- `SECRET_KEY`: JWT secret (generate with `openssl rand -hex 32`)
- `GOOGLE_CLIENT_ID`: Google OAuth client ID (optional)
- `GOOGLE_CLIENT_SECRET`: Google OAuth client secret (optional)

### 4. Initialize Database

```bash
python init_db.py
```

This will:
- Create all database tables
- Seed 5 default job sections

### 5. Run the Server

```bash
# Development
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Production
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

API will be available at:
- **API**: http://localhost:8000
- **Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ main.py                 # FastAPI app entry point
â”œâ”€â”€ config.py              # Settings and configuration
â”œâ”€â”€ database.py            # Database connection
â”œâ”€â”€ models.py              # SQLAlchemy models
â”œâ”€â”€ schemas.py             # Pydantic schemas
â”œâ”€â”€ init_db.py             # Database initialization
â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ jwt_handler.py     # JWT token management
â”‚   â”œâ”€â”€ google_oauth.py    # Google OAuth verification
â”‚   â””â”€â”€ dependencies.py    # Auth dependencies
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ pdf_extractor.py   # PDF text extraction
â”‚   â””â”€â”€ chunker.py         # Custom chunking logic
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ model_manager.py   # OpenAI embedding model
â”‚   â””â”€â”€ vector_store.py    # ChromaDB vector store
â”œâ”€â”€ scoring/
â”‚   â””â”€â”€ ranking_engine.py  # Deterministic ranking
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ explainer.py       # RAG-based explanations
â””â”€â”€ routes/
    â”œâ”€â”€ auth.py            # Authentication endpoints
    â”œâ”€â”€ jobs.py            # Job management endpoints
    â”œâ”€â”€ resumes.py         # Resume upload endpoints
    â””â”€â”€ ranking.py         # Ranking & explanation endpoints
```

## ğŸ”Œ API Endpoints

### Authentication
- `POST /auth/register` - Register with email/password
- `POST /auth/login` - Login with email/password
- `POST /auth/google` - Login with Google OAuth

### Jobs
- `GET /jobs/sections` - Get all job sections
- `GET /jobs/section/{name}` - Get jobs in a section
- `GET /jobs/{id}` - Get job details
- `POST /jobs/` - Create job (admin)
- `PUT /jobs/{id}` - Update job (admin)
- `DELETE /jobs/{id}` - Delete job (admin)
- `GET /jobs/{id}/documents` - Get job documents
- `POST /jobs/{id}/documents` - Upload job document (admin)

### Resumes
- `POST /resumes/upload` - Upload resume
- `GET /resumes/my-resumes` - Get user's resumes
- `GET /resumes/job/{id}` - Get resumes for job (admin)

### Ranking
- `POST /ranking/rank` - Run ranking for a job (admin)
- `GET /ranking/results/{job_id}` - Get ranking results (admin)
- `GET /ranking/my-results` - Get user's results
- `GET /ranking/explanation/{result_id}` - Get RAG explanation

## ğŸ¯ Ranking Algorithm

**Weighted Scoring:**
- Skills: 40%
- Experience: 30%
- Education: 15%
- Projects: 15%

**Process:**
1. Extract text from PDF resume
2. Detect sections (skills, experience, education, projects)
3. Chunk text semantically
4. Generate embeddings using OpenAI text-embedding-3-small
5. Store in ChromaDB vector database
6. Calculate cosine similarity with job requirements
7. Apply weighted scoring formula
8. Generate RAG explanation using LangChain + GPT-3.5-turbo

## ğŸ” Google OAuth Setup

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project
3. Enable Google+ API
4. Create OAuth 2.0 credentials
5. Add authorized redirect URIs:
   - `http://localhost:3000/api/auth/callback/google`
6. Copy Client ID and Client Secret to `.env`

## ğŸ—„ï¸ Database Schema

- **users**: User accounts (email, password, Google ID, role)
- **job_sections**: 5 predefined job categories
- **job_roles**: Job postings within sections
- **job_documents**: Documents attached to jobs
- **resumes**: Uploaded resumes
- **ranking_results**: Scoring results with explanations

## ğŸ§ª Testing

```bash
# Run tests
pytest

# With coverage
pytest --cov=. --cov-report=html
```

## ğŸš€ Deployment

### Using Docker

```bash
docker build -t ats-backend .
docker run -p 8000:8000 --env-file .env ats-backend
```

### Using Railway/Render

1. Connect your GitHub repository
2. Set environment variables
3. Deploy with auto-scaling

## ğŸ“ License

MIT License
# Resume-ATS-Score-Checker
